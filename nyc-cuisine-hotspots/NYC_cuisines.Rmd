---
title: "Finding NYC Cuisine Hotspots with DBSCAN Clustering"
author: "Evan Powell"
date: "2025-08-08"
output: html_document
---

## 1. Abstract
This project analyzes New York City's restaurant inspection data from the NYC Department of Health and Mental Hygiene (NYDOHMH) to identify cuisine hotspots across the five boroughs. Using DBSCAN clustering, we map regions with a high concentration of specific cuisines and explore how these patterns vary spatially.  
The results are visualized with interactive maps, providing insights into where New Yorkers and visitors can find culinary clusters across the city.


## 2. Setup & Dependencies

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
# Load packages
library(tidyverse)
library(janitor)
library(sf)
library(dbscan)
library(tmap)
library(leaflet)
library(tidycensus)
library(tigris)
library(skimr)
library(RSocrata)
library(RColorBrewer)
library(dplyr)
library(rmapshaper)
```

## 3. Data Acquisition

```{r reading nyc data}
#read data from NYC Open Data
restaurants <- read.socrata(
  "https://data.cityofnewyork.us/resource/43nn-pn8j.json",
  app_token = Sys.getenv("SOCRATA_APP_TOKEN"),
  email     = Sys.getenv("SOCRATA_EMAIL"),
  password  = Sys.getenv("SOCRATA_PASSWORD")
)
```

``` {r local save}
saveRDS(restaurants, "restaurants.rds")

```
```{r local retrieve}
restaurants <- readRDS("restaurants.rds")
```

## 4. Data Cleaning

```{r clean data}
#Remove columns we don't need
restaurants <- restaurants %>%
  select(camis, dba, boro, cuisine_description, latitude, longitude)

#Make numeric the Lat / Long.
restaurants <- restaurants %>%
  mutate(
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude)
  )

#drop rows without Lat / Long. or cuisine description
restaurants <- restaurants %>%
  filter(
    !is.na(latitude),
    !is.na(longitude),
    !is.na(cuisine_description)
  )

#deduplicate
restaurants_unique <- restaurants %>%
  distinct(camis, .keep_all = TRUE)
```

## 5. Helper Functions

### 5.1 Listing Cuisines

```{r listing cuisines}
list_cuisines <- function(data) {
  data %>%
    group_by(cuisine_description) %>%
    summarise(n = n(), .groups = "drop") %>%
    arrange(desc(n))
}

cuisine_counts <- list_cuisines(restaurants_unique)
```

### 5.2 Basic DBSCAN Clustering

Two functions: one for basic DBSCAN, one that runs 'auto' to adjust for varying presences of each cuisine across the city.

```{r clustering}
run_dbscan_for_cuisine <- function(data, cuisine, eps = 0.005, minPts = 5) {
 # 1. Filter for the chosen cuisine and remove null island coords
  cuisine_data <- data %>%
    filter(
      cuisine_description == cuisine,
      !is.na(latitude),
      !is.na(longitude),
      latitude != 0,
      longitude != 0
    )
 # 2. Prepare coordinates matrix
  coords <- cuisine_data %>%
    select(longitude, latitude) %>%
    as.matrix()
  # 3. Run DBSCAN
  db_result <- dbscan::dbscan(coords, eps = eps, minPts = minPts)
  # 4. Add cluster assignments back to data
  cuisine_data <- cuisine_data %>%
    mutate(cluster = db_result$cluster)
  
  return(cuisine_data)
}

run_dbscan_for_cuisine_auto <- function(data, cuisine, eps = 0.009, min_floor = 3, prop_factor = 0.02) {
 # 1. Filter for the chosen cuisine and remove null island coords
  cuisine_data <- data %>%
    filter(
      cuisine_description == cuisine,
      !is.na(latitude),
      !is.na(longitude),
      latitude != 0,
      longitude != 0
    )
  
  total_count <- nrow(cuisine_data)
  if (total_count == 0) {
    warning("No restaurants found for cuisine: ", cuisine)
    return(NULL)
  }
  
  # 2. Calculate minPts dynamically
  minPts <- max(min_floor, round(total_count * prop_factor))
    
 # 3. Prepare coordinates matrix
  coords <- cuisine_data %>%
    select(longitude, latitude) %>%
    as.matrix()
  # 3. Run DBSCAN
  db_result <- dbscan::dbscan(coords, eps = eps, minPts = minPts)
  # 5. Add cluster assignments back to data
  cuisine_data <- cuisine_data %>%
    mutate(cluster = db_result$cluster)
  
  # Attach settings used
  attr(cuisine_data, "dbscan_params") <- list(
    eps = eps,
    minPts = minPts,
    total_count = total_count
  )
  
  return(cuisine_data)
}

```

### 5.3 Clustering by Borough

```{r clustering by boro}
run_dbscan_by_borough <- function(data, cuisine,
                                  eps_values= c("Manhattan" = 0.004, 
                                                "Brooklyn/Queens" = 0.0078, 
                                                "Bronx" = 0.007, 
                                                "Staten Island" = 0.008),
                                  min_floor = 4, 
                                  prop_factor = 0.015) {
  
  # eps_values = named vector with borough groupings
  # e.g., c("Manhattan" = 0.004, "Brooklyn/Queens" = 0.007, "Bronx" = 0.006, "Staten Island" = 0.008)
  
  # Filter for chosen cuisine & clean coords
  cuisine_data <- data %>%
    filter(
      cuisine_description == cuisine,
      !is.na(latitude),
      !is.na(longitude),
      latitude != 0,
      longitude != 0
    )
  
  # Dynamic minPts based on citywide restaurant count
  total_count <- nrow(cuisine_data)
  minPts <- max(min_floor, round(total_count * prop_factor))
  
  # Group boroughs
  cuisine_data <- cuisine_data %>%
    mutate(boro_group = case_when(
      boro == "Manhattan" ~ "Manhattan",
      boro %in% c("Brooklyn", "Queens") ~ "Brooklyn/Queens",
      boro == "Bronx" ~ "Bronx",
      boro == "Staten Island" ~ "Staten Island",
      TRUE ~ "Other"
    ))
  
  all_results <- list()
  cluster_offset <- 0
  
  # Loop through each borough group
  for (grp in unique(cuisine_data$boro_group)) {
    eps <- eps_values[[grp]]
    
    sub_data <- cuisine_data %>% filter(boro_group == grp)
    
    if (nrow(sub_data) == 0) next
    
    
    coords <- sub_data %>%
      select(longitude, latitude) %>%
      as.matrix()
    
    db_result <- dbscan::dbscan(coords, eps = eps, minPts = minPts)
    
    # Offset cluster IDs so they are unique across boroughs
    sub_data <- sub_data %>%
      mutate(cluster = ifelse(db_result$cluster == 0, 0, db_result$cluster + cluster_offset))
    
    # Update offset for next borough
    cluster_offset <- max(max(sub_data$cluster, na.rm = TRUE), cluster_offset)
    
    all_results[[grp]] <- sub_data
  }
  
  final_data <- bind_rows(all_results)
  
  return(final_data)
}

```

### 5.4 Mapping Results

``` {r mapping}
map_dbscan_clusters <- function(clustered_data) {
  
  clustered_data <- clustered_data %>%
    filter(latitude != 0, longitude != 0) %>%
    mutate(cluster = as.factor(cluster))
  
  clustered_sf <- st_as_sf(
    clustered_data,
    coords = c("longitude", "latitude"),
    crs = 4326,   # WGS84 lat/long
    remove = FALSE
  )
  
  # Dynamically create enough colors for the number of clusters
  num_clusters <- length(unique(clustered_sf$cluster))
  colors <- brewer.pal(
    n = max(3, min(12, num_clusters)),  # palettes max out at 12 colors
    name = "Set1"
  )
  
  tmap_mode("view")  # interactive mode
  
  tm_shape(clustered_sf) +
    tm_dots(
      col = "cluster",        # color by cluster ID
      palette = colors,       # distinct colors
      size = 0.55,            # small points
      alpha = 0.7,            # semi-transparent
      title = "Cluster ID"
    ) +
    tm_layout(legend.outside = TRUE)
  
}
```

### 5.5 Polygon "Region" Creation

``` {r regions}

# hotspot polygon builder
make_hotspot_polygons <- function(clustered_data,
                                  buffer_dist_m = 100,   # widen polygons by this many meters
                                  smooth = TRUE,         # apply small smoothing (buffer+unbuffer)
                                  smooth_frac = 0.5,     # smoothing distance as fraction of buffer_dist_m
                                  proj_crs = 3857) {     # projected CRS for metric ops (Web Mercator)
  # clustered_data: data.frame/tibble with longitude, latitude, cluster (0 = noise), and any other cols
  # Returns: sf with polygons in WGS84 (EPSG:4326) and a 'count' column (# restaurants in region)
  
  # 1) Make sure we have points and non-zero clusters
  clustered_sf <- st_as_sf(clustered_data,
                           coords = c("longitude", "latitude"),
                           crs = 4326,
                           remove = FALSE)
  
  pts <- clustered_sf %>% filter(!is.na(cluster) & cluster != 0)

  
  # 2) Build convex hull per DBSCAN cluster (explicit geometry creation inside summarise)
  hulls <- pts %>%
    group_by(cluster) %>%
    summarise(
      count = n(),
      geometry = st_convex_hull(st_combine(geometry)),  # make one polygon per cluster
      .groups = "drop"
    )
  
  # 3) Project to metric CRS for buffering/union
  hulls_proj <- st_transform(hulls, crs = proj_crs)
  
  # 4) Buffer (widen thin shapes)
  hulls_buffed <- st_buffer(hulls_proj, dist = buffer_dist_m)
  
  # 5) Merge overlapping buffered hulls into one or more polygons
  merged_union <- st_union(hulls_buffed)                 # returns sfc
  # Cast union result into POLYGONs (may be MULTIPOLYGON -> POLYGON pieces)
  merged_polys <- st_cast(merged_union, "POLYGON")
  merged_sf <- st_as_sf(data.frame(id = seq_along(merged_polys)), geometry = merged_polys)
  st_crs(merged_sf) <- proj_crs
  
  # 6) For each merged polygon, compute total counts by summing counts of hulls that intersect it
  inter_list <- st_intersects(merged_sf, hulls_proj)
  counts <- sapply(inter_list, function(ix) {
    if (length(ix) == 0) return(0L)
    sum(hulls_proj$count[ix], na.rm = TRUE)
  })
  merged_sf$count <- counts
  
  # 7) Optional smoothing: small positive buffer then negative buffer to round corners
  if (isTRUE(smooth) && buffer_dist_m > 0) {
    smooth_dist <- buffer_dist_m * smooth_frac
    # small positive buffer then negative buffer; do it in projected CRS
    merged_sf <- st_buffer(merged_sf, smooth_dist)
    merged_sf <- st_buffer(merged_sf, -smooth_dist)
    # if buffering removed geometry because polygon too small, keep original in that case
  }
  
  # 8) Transform back to WGS84 for mapping / returning
  result <- st_transform(merged_sf, crs = 4326)
  
  # 9) Add a human-friendly label column (optional)
  result <- result %>% mutate(label = paste0("hotspot_", id))
  
  return(result)
}

```
  
  Another mapping function is needed for the regions:
  
``` {r mapping regions}  
map_cluster_polygons <- function(hulls) {
  
  if (nrow(hulls) == 0) {
    print("No hotspot polygons to display.")
    return(invisible(NULL))
  }
  
  tmap_mode("view")
  tm_shape(hulls) +
  tm_polygons(col = "count", palette = "YlOrRd", alpha = 0.5, title = "Restaurants in hotspot") +
  tm_shape(st_as_sf(hulls, coords = c("longitude","latitude"), crs = 4326)) 

}
```

## 6. Example Analyses

### 6.1 Sample without Borough-level clustering
``` {r example Caribbean}
caribbean_clusters <- run_dbscan_for_cuisine_auto(
  data = restaurants_unique,
  cuisine = "Caribbean",
)

# head(caribbean_clusters)

map_dbscan_clusters(caribbean_clusters)
```

### 6.2 Sample with Borough-level clustering


```{r example boro any}

eps_settings <- c(
  "Manhattan" = 0.004,
  "Brooklyn/Queens" = 0.008,
  "Bronx" = 0.007,
  "Staten Island" = 0.009
)

boro_test_cuisine = "Latin American"

test2_clusters <- run_dbscan_by_borough(
  data = restaurants_unique,
  cuisine = boro_test_cuisine,
  eps_values = eps_settings,
  min_floor = 5,
  prop_factor = 0.015
)

map_dbscan_clusters(test2_clusters)

```

### 6.3 Sample of Polygon Regions

``` {r test regions}

region_test_cuisine = "Chinese"

region_test_clusters <- run_dbscan_by_borough(
  data = restaurants_unique,
  cuisine = region_test_cuisine,
  eps_values = eps_settings,
  min_floor = 5,
  prop_factor = 0.015
)

region_test_hulls <- make_hotspot_polygons(region_test_clusters)

map_cluster_polygons(region_test_hulls)
```

