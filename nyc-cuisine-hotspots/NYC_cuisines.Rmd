---
title: "NYC Cuisine Hotspots"
author: "Evan Powell"
date: "2025-08-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
# Load packages
library(tidyverse)
library(janitor)
library(sf)
library(dbscan)
library(tmap)
library(leaflet)
library(tidycensus)
library(tigris)
library(skimr)
library(RSocrata)
library(RColorBrewer)
library(dplyr)
library(rmapshaper)
```

```{r reading nyc data}
#read data from NYC Open Data
restaurants <- read.socrata(
  "https://data.cityofnewyork.us/resource/43nn-pn8j.json",
  app_token = Sys.getenv("SOCRATA_APP_TOKEN"),
  email     = Sys.getenv("SOCRATA_EMAIL"),
  password  = Sys.getenv("SOCRATA_PASSWORD")
)
```

``` {r local save}
saveRDS(restaurants, "restaurants.rds")

```
```{r local retrieve}
restaurants <- readRDS("restaurants.rds")
```

```{r clean data}
#Remove columns we don't need
restaurants <- restaurants %>%
  select(camis, dba, boro, cuisine_description, latitude, longitude)

#Make numeric the Lat / Long.
restaurants <- restaurants %>%
  mutate(
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude)
  )

#drop rows without Lat / Long. or cuisine description
restaurants <- restaurants %>%
  filter(
    !is.na(latitude),
    !is.na(longitude),
    !is.na(cuisine_description)
  )

#deduplicate
restaurants_unique <- restaurants %>%
  distinct(camis, .keep_all = TRUE)
```

```{r listing cuisines}
list_cuisines <- function(data) {
  data %>%
    group_by(cuisine_description) %>%
    summarise(n = n(), .groups = "drop") %>%
    arrange(desc(n))
}

cuisine_counts <- list_cuisines(restaurants_unique)
```

## Basic Clustering

```{r clustering}
run_dbscan_for_cuisine <- function(data, cuisine, eps = 0.005, minPts = 5) {
 # 1. Filter for the chosen cuisine and remove null island coords
  cuisine_data <- data %>%
    filter(
      cuisine_description == cuisine,
      !is.na(latitude),
      !is.na(longitude),
      latitude != 0,
      longitude != 0
    )
 # 2. Prepare coordinates matrix
  coords <- cuisine_data %>%
    select(longitude, latitude) %>%
    as.matrix()
  # 3. Run DBSCAN
  db_result <- dbscan::dbscan(coords, eps = eps, minPts = minPts)
  # 4. Add cluster assignments back to data
  cuisine_data <- cuisine_data %>%
    mutate(cluster = db_result$cluster)
  
  return(cuisine_data)
}

run_dbscan_for_cuisine_auto <- function(data, cuisine, eps = 0.005, min_floor = 3, prop_factor = 0.02) {
 # 1. Filter for the chosen cuisine and remove null island coords
  cuisine_data <- data %>%
    filter(
      cuisine_description == cuisine,
      !is.na(latitude),
      !is.na(longitude),
      latitude != 0,
      longitude != 0
    )
  
  total_count <- nrow(cuisine_data)
  if (total_count == 0) {
    warning("No restaurants found for cuisine: ", cuisine)
    return(NULL)
  }
  
  # 2. Calculate minPts dynamically
  minPts <- max(min_floor, round(total_count * prop_factor))
    
 # 3. Prepare coordinates matrix
  coords <- cuisine_data %>%
    select(longitude, latitude) %>%
    as.matrix()
  # 3. Run DBSCAN
  db_result <- dbscan::dbscan(coords, eps = eps, minPts = minPts)
  # 5. Add cluster assignments back to data
  cuisine_data <- cuisine_data %>%
    mutate(cluster = db_result$cluster)
  
  # Attach settings used
  attr(cuisine_data, "dbscan_params") <- list(
    eps = eps,
    minPts = minPts,
    total_count = total_count
  )
  
  return(cuisine_data)
}

```


``` {r mapping}
map_dbscan_clusters <- function(clustered_data) {
  
  clustered_data <- clustered_data %>%
    filter(latitude != 0, longitude != 0) %>%
    mutate(cluster = as.factor(cluster))
  
  clustered_sf <- st_as_sf(
    clustered_data,
    coords = c("longitude", "latitude"),
    crs = 4326,   # WGS84 lat/long
    remove = FALSE
  )
  
  # Dynamically create enough colors for the number of clusters
  num_clusters <- length(unique(clustered_sf$cluster))
  colors <- brewer.pal(
    n = max(3, min(12, num_clusters)),  # palettes max out at 12 colors
    name = "Set1"
  )
  
  tmap_mode("view")  # interactive mode
  
  tm_shape(clustered_sf) +
    tm_dots(
      col = "cluster",        # color by cluster ID
      palette = colors,       # distinct colors
      size = 0.55,            # small points
      alpha = 0.7,            # semi-transparent
      title = "Cluster ID"
    ) +
    tm_layout(legend.outside = TRUE)
  
}
```

``` {r example Caribbean}
caribbean_clusters <- run_dbscan_for_cuisine(
  data = restaurants_unique,
  cuisine = "Caribbean",
  eps = 0.0075,  
  minPts = 5
)

head(caribbean_clusters)

map_dbscan_clusters(caribbean_clusters)
```


``` {r example Chinese}
chinese_clusters <- run_dbscan_for_cuisine(
  data = restaurants_unique,
  cuisine = "Chinese",
  eps = 0.005,  
  minPts = 9
)


map_dbscan_clusters(chinese_clusters)
```

``` {r example any}

test_cuisine = "Steakhouse"

test_cuisine_clusters <- run_dbscan_for_cuisine_auto(
  data = restaurants_unique,
  cuisine = test_cuisine,
  eps = 0.007,  
  min_floor = 4,
  prop_factor = 0.01
)


map_dbscan_clusters(test_cuisine_clusters)
```


## Clustering with Respect to Borough Densities

```{r clustering by boro}
run_dbscan_by_borough <- function(data, cuisine, eps_values, min_floor = 3, prop_factor = 0.02) {
  
  # eps_values = named vector with borough groupings
  # e.g., c("Manhattan" = 0.0025, "Brooklyn/Queens" = 0.005, "Bronx" = 0.006, "Staten Island" = 0.008)
  
  # Filter for chosen cuisine & clean coords
  cuisine_data <- data %>%
    filter(
      cuisine_description == cuisine,
      !is.na(latitude),
      !is.na(longitude),
      latitude != 0,
      longitude != 0
    )
  
  # Dynamic minPts based on citywide restaurant count
  total_count <- nrow(cuisine_data)
  minPts <- max(min_floor, round(total_count * prop_factor))
  
  # Group boroughs
  cuisine_data <- cuisine_data %>%
    mutate(boro_group = case_when(
      boro == "Manhattan" ~ "Manhattan",
      boro %in% c("Brooklyn", "Queens") ~ "Brooklyn/Queens",
      boro == "Bronx" ~ "Bronx",
      boro == "Staten Island" ~ "Staten Island",
      TRUE ~ "Other"
    ))
  
  all_results <- list()
  cluster_offset <- 0
  
  # Loop through each borough group
  for (grp in unique(cuisine_data$boro_group)) {
    eps <- eps_values[[grp]]
    
    sub_data <- cuisine_data %>% filter(boro_group == grp)
    
    if (nrow(sub_data) == 0) next
    
    
    coords <- sub_data %>%
      select(longitude, latitude) %>%
      as.matrix()
    
    db_result <- dbscan::dbscan(coords, eps = eps, minPts = minPts)
    
    # Offset cluster IDs so they are unique across boroughs
    sub_data <- sub_data %>%
      mutate(cluster = ifelse(db_result$cluster == 0, 0, db_result$cluster + cluster_offset))
    
    # Update offset for next borough
    cluster_offset <- max(max(sub_data$cluster, na.rm = TRUE), cluster_offset)
    
    all_results[[grp]] <- sub_data
  }
  
  final_data <- bind_rows(all_results)
  
  return(final_data)
}

```

```{r example boro any}

eps_settings <- c(
  "Manhattan" = 0.004,
  "Brooklyn/Queens" = 0.007,
  "Bronx" = 0.006,
  "Staten Island" = 0.008
)

boro_test_cuisine = "Afghan"

test2_clusters <- run_dbscan_by_borough(
  data = restaurants_unique,
  cuisine = boro_test_cuisine,
  eps_values = eps_settings,
  min_floor = 5,
  prop_factor = 0.015
)

map_dbscan_clusters(test2_clusters)

```

## Creating Regions from Clusters

``` {r regions}

# hotspot polygon builder
make_hotspot_polygons <- function(clustered_data,
                                  buffer_dist_m = 100,   # widen polygons by this many meters
                                  smooth = TRUE,         # apply small smoothing (buffer+unbuffer)
                                  smooth_frac = 0.5,     # smoothing distance as fraction of buffer_dist_m
                                  proj_crs = 3857) {     # projected CRS for metric ops (Web Mercator)
  # clustered_data: data.frame/tibble with longitude, latitude, cluster (0 = noise), and any other cols
  # Returns: sf with polygons in WGS84 (EPSG:4326) and a 'count' column (# restaurants in region)
  
  # 1) Make sure we have points and non-zero clusters
  clustered_sf <- st_as_sf(clustered_data,
                           coords = c("longitude", "latitude"),
                           crs = 4326,
                           remove = FALSE)
  
  pts <- clustered_sf %>% filter(!is.na(cluster) & cluster != 0)

  
  # 2) Build convex hull per DBSCAN cluster (explicit geometry creation inside summarise)
  hulls <- pts %>%
    group_by(cluster) %>%
    summarise(
      count = n(),
      geometry = st_convex_hull(st_combine(geometry)),  # make one polygon per cluster
      .groups = "drop"
    )
  
  # 3) Project to metric CRS for buffering/union
  hulls_proj <- st_transform(hulls, crs = proj_crs)
  
  # 4) Buffer (widen thin shapes)
  hulls_buffed <- st_buffer(hulls_proj, dist = buffer_dist_m)
  
  # 5) Merge overlapping buffered hulls into one or more polygons
  merged_union <- st_union(hulls_buffed)                 # returns sfc
  # Cast union result into POLYGONs (may be MULTIPOLYGON -> POLYGON pieces)
  merged_polys <- st_cast(merged_union, "POLYGON")
  merged_sf <- st_as_sf(data.frame(id = seq_along(merged_polys)), geometry = merged_polys)
  st_crs(merged_sf) <- proj_crs
  
  # 6) For each merged polygon, compute total counts by summing counts of hulls that intersect it
  inter_list <- st_intersects(merged_sf, hulls_proj)
  counts <- sapply(inter_list, function(ix) {
    if (length(ix) == 0) return(0L)
    sum(hulls_proj$count[ix], na.rm = TRUE)
  })
  merged_sf$count <- counts
  
  # 7) Optional smoothing: small positive buffer then negative buffer to round corners
  if (isTRUE(smooth) && buffer_dist_m > 0) {
    smooth_dist <- buffer_dist_m * smooth_frac
    # small positive buffer then negative buffer; do it in projected CRS
    merged_sf <- st_buffer(merged_sf, smooth_dist)
    merged_sf <- st_buffer(merged_sf, -smooth_dist)
    # if buffering removed geometry because polygon too small, keep original in that case
  }
  
  # 8) Transform back to WGS84 for mapping / returning
  result <- st_transform(merged_sf, crs = 4326)
  
  # 9) Add a human-friendly label column (optional)
  result <- result %>% mutate(label = paste0("hotspot_", id))
  
  return(result)
}


  
map_cluster_polygons <- function(hulls) {
  
  if (nrow(hulls) == 0) {
    print("No hotspot polygons to display.")
    return(invisible(NULL))
  }
  
  tmap_mode("view")
  tm_shape(hulls) +
  tm_polygons(col = "count", palette = "YlOrRd", alpha = 0.5, title = "Restaurants in hotspot") +
  tm_shape(st_as_sf(hulls, coords = c("longitude","latitude"), crs = 4326)) 

}
```

``` {r test regions}

region_test_cuisine = "African"

region_test_clusters <- run_dbscan_by_borough(
  data = restaurants_unique,
  cuisine = region_test_cuisine,
  eps_values = eps_settings,
  min_floor = 5,
  prop_factor = 0.015
)

region_test_hulls <- make_hotspot_polygons(region_test_clusters)

map_cluster_polygons(region_test_hulls)
```

