---
title: "Finding NYC Cuisine Hotspots with DBSCAN Clustering"
author: "Evan Powell"
date: "2025-08-08"
output: html_document
---

## 1. Abstract

This project analyzes the geographic distribution of cuisines in New York City using data from the NYC Department of Health and Mental Hygiene’s restaurant inspection dataset. Each restaurant is classified by cuisine type and geocoded by latitude and longitude, enabling a spatial exploration of food diversity across the five boroughs.

To identify neighborhoods with a high density of specific cuisines, we apply DBSCAN, a density-based clustering algorithm that automatically detects hotspots without requiring a predefined number of clusters. These clusters are then visualized using geospatial mapping, and further summarized into polygon “hotspot regions” that represent cuisine neighborhoods at a glance.

In addition to visualization, we calculate quantitative measures of concentration and diffusion, including the share of restaurants in clusters, the size of the largest cluster, and a normalized entropy-based diffusion score. These metrics allow us to compare cuisines analytically — distinguishing those tied to specific neighborhoods (e.g., Italian in Little Italy, Greek in Astoria) from those widely available throughout the city (e.g., American diners, pizza). Together, these methods demonstrate how geospatial clustering and exploratory data science techniques can uncover meaningful patterns in cultural and economic life, illustration in this project the diversity of cuisine available in New York.


## 2. Setup & Dependencies

This section loads all required R packages for data handling, spatial analysis, clustering, and visualization.  
By loading them here, we ensure that the rest of the analysis runs smoothly without needing repeated `library()` calls in later chunks.  
Packages are grouped by purpose:
- **Data wrangling & cleaning:** `tidyverse`, `janitor`, `dplyr`, `skimr`, `purrr`  
- **Spatial data handling:** `sf`, `tigris`, `tidycensus`  
- **Clustering:** `dbscan`  
- **Mapping:** `tmap`, `leaflet`, `RColorBrewer`, `rmapshaper`  
- **Data acquisition:** `RSocrata` (to pull data from NYC Open Data)  
We also disable message and warning output to keep the knitted report clean and readable.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
# Load packages
library(tidyverse)
library(janitor)
library(sf)
library(dbscan)
library(tmap)
library(leaflet)
library(tidycensus)
library(tigris)
library(skimr)
library(RSocrata)
library(RColorBrewer)
library(dplyr)
library(rmapshaper)
library(purrr)
library(kableExtra)
library(tidyr)
library(FactoMineR)
library(factoextra)
library(Rtsne)
```

## 3. Data Acquisition

Here we retrieve the NYC restaurant inspection dataset from the NYC Open Data API.  
We use **RSocrata** to connect directly to the API, authenticating with a token and credentials stored in environment variables to keep sensitive information private.  

The data is saved locally as an `.rds` file so we can reload it instantly without having to re-download from the API every time we run the analysis (which will require an app token).


```{r reading nyc data}
#read data from NYC Open Data
restaurants <- read.socrata(
  "https://data.cityofnewyork.us/resource/43nn-pn8j.json",
  app_token = Sys.getenv("SOCRATA_APP_TOKEN"),
  email     = Sys.getenv("SOCRATA_EMAIL"),
  password  = Sys.getenv("SOCRATA_PASSWORD")
)
```

``` {r local save}
saveRDS(restaurants, "restaurants.rds")

```
```{r local retrieve}
restaurants <- readRDS("restaurants.rds")
```

## 4. Data Cleaning

The raw dataset contains many columns we don’t need for our clustering analysis, so we first keep only:
- `camis` (unique restaurant ID)
- `dba` (restaurant name)
- `boro` (borough name)
- `cuisine_description` (type of cuisine served)
- `latitude` / `longitude` (restaurant location)

We then:
1. Convert latitude and longitude from strings to numeric values.
2. Remove rows without valid coordinates or a cuisine description.
3. Deduplicate by `camis` to ensure each restaurant appears only once (since the raw data has one row per inspection, not per restaurant).

The cleaned dataset, `restaurants_unique`, is now ready for analysis.

```{r clean data}
#Remove columns we don't need
restaurants <- restaurants %>%
  select(camis, dba, boro, cuisine_description, latitude, longitude)

#Make numeric the Lat / Long.
restaurants <- restaurants %>% 
  mutate(
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude)
  )

#drop rows without Lat / Long. or cuisine description
restaurants <- restaurants %>%
  filter(
    !is.na(latitude),
    !is.na(longitude),
    !is.na(cuisine_description)
  )

#deduplicate
restaurants_unique <- restaurants %>%
  distinct(camis, .keep_all = TRUE)
```

The resulting dataset gives us one row per restaurant, suitable for geospatial clustering.

Note on cleaned data: while we can remove duplicate restaurants and isolate the relevant columns, issues persist with the cuisine labeling of the data. There is no consistent standards to determine the label: restaurants themselves select these; and several categories are subcategories of others (e.g., 'Peruvain' and 'Latin American' or 'Thai' and 'Southeast Asian')

## 5. Helper Functions

### 5.1 Listing Cuisines

This function generates a sorted list of all cuisine types in the dataset along with the number of unique restaurants serving each cuisine.  
It’s useful for:
- Understanding the diversity of cuisines in NYC.
- Selecting which cuisines to run clustering on (e.g., choosing only those with enough restaurants to form meaningful clusters).


```{r listing cuisines}
list_cuisines <- function(data) {
  data %>%
    group_by(cuisine_description) %>%
    summarise(n = n(), .groups = "drop") %>%
    arrange(desc(n))
}

cuisine_counts <- list_cuisines(restaurants_unique)

kable(cuisine_counts, caption = "No. Restaurants by Cuisine", format = "html")

```

As expected, categories such as American, Chinese, and Pizza are among the most common, while more specialized cuisines appear only rarely.

### 5.2 Basic DBSCAN Clustering

To identify hotspots of a given cuisine, we apply DBSCAN, a density-based clustering algorithm well-suited for geographic data. Unlike k-means, DBSCAN does not require specifying the number of clusters in advance and can flexibly separate dense neighborhoods from background noise.

There are functions: one for basic DBSCAN, one that runs 'auto' to adjust for varying presences of each cuisine across the city. This is done by modifying `minPts`, the number of points needed to form a cluster, i.e., the number of restaurants needed to be considered a hotspot of that cuisine. 

```{r clustering}
run_dbscan_for_cuisine <- function(data, cuisine, eps = 0.005, minPts = 5) {
 # 1. Filter for the chosen cuisine and remove null island coords
  cuisine_data <- data %>%
    filter(
      cuisine_description == cuisine,
      !is.na(latitude),
      !is.na(longitude),
      latitude != 0,
      longitude != 0
    )
 # 2. Prepare coordinates matrix
  coords <- cuisine_data %>%
    select(longitude, latitude) %>%
    as.matrix()
  # 3. Run DBSCAN
  db_result <- dbscan::dbscan(coords, eps = eps, minPts = minPts)
  # 4. Add cluster assignments back to data
  cuisine_data <- cuisine_data %>%
    mutate(cluster = db_result$cluster)
  
  return(cuisine_data)
}

run_dbscan_for_cuisine_auto <- function(data, cuisine, eps = 0.009, min_floor = 3, prop_factor = 0.02) {
 # 1. Filter for the chosen cuisine and remove null island coords
  cuisine_data <- data %>%
    filter(
      cuisine_description == cuisine,
      !is.na(latitude),
      !is.na(longitude),
      latitude != 0,
      longitude != 0
    )
  
  total_count <- nrow(cuisine_data)
  if (total_count == 0) {
    warning("No restaurants found for cuisine: ", cuisine)
    return(NULL)
  }
  
  # 2. Calculate minPts dynamically
  minPts <- max(min_floor, round(total_count * prop_factor))
    
 # 3. Prepare coordinates matrix
  coords <- cuisine_data %>%
    select(longitude, latitude) %>%
    as.matrix()
  # 3. Run DBSCAN
  db_result <- dbscan::dbscan(coords, eps = eps, minPts = minPts)
  # 5. Add cluster assignments back to data
  cuisine_data <- cuisine_data %>%
    mutate(cluster = db_result$cluster)
  
  # Attach settings used
  attr(cuisine_data, "dbscan_params") <- list(
    eps = eps,
    minPts = minPts,
    total_count = total_count
  )
  
  return(cuisine_data)
}

```

Each restaurant is assigned either to a cluster (positive ID) or to noise (ID 0). The results highlight natural concentrations of restaurants without imposing artificial boundaries.

### 5.3 Clustering by Borough

This function runs the **DBSCAN** clustering algorithm on restaurants serving a chosen cuisine.  
Because restaurant density varies greatly by borough (e.g., Manhattan vs. Staten Island), we run DBSCAN separately for different borough groupings with customized `eps` values.

Key points:
- `eps` controls the maximum distance between points to be considered part of the same cluster.
- `minPts` is set dynamically based on the total number of restaurants for the cuisine — this ensures that common cuisines need more restaurants to form a hotspot, while rarer cuisines can form clusters with fewer points.
- We offset cluster IDs by borough so clusters remain uniquely identifiable.

The output is a dataframe of restaurants with an assigned cluster ID (or noise label `0`).


```{r clustering by boro}
run_dbscan_by_borough <- function(data, cuisine,
                                  eps_values= c("Manhattan" = 0.004, 
                                                "Brooklyn/Queens" = 0.0078, 
                                                "Bronx" = 0.007, 
                                                "Staten Island" = 0.008),
                                  min_floor = 4, 
                                  prop_factor = 0.015) {
  
  # eps_values = named vector with borough groupings
  # e.g., c("Manhattan" = 0.004, "Brooklyn/Queens" = 0.007, "Bronx" = 0.006, "Staten Island" = 0.008)
  
  # Filter for chosen cuisine & clean coords
  cuisine_data <- data %>%
    filter(
      cuisine_description == cuisine,
      !is.na(latitude),
      !is.na(longitude),
      latitude != 0,
      longitude != 0
    )
  
  # Dynamic minPts based on citywide restaurant count
  total_count <- nrow(cuisine_data)
  minPts <- max(min_floor, round(total_count * prop_factor))
  
  # Group boroughs
  cuisine_data <- cuisine_data %>%
    mutate(boro_group = case_when(
      boro == "Manhattan" ~ "Manhattan",
      boro %in% c("Brooklyn", "Queens") ~ "Brooklyn/Queens",
      boro == "Bronx" ~ "Bronx",
      boro == "Staten Island" ~ "Staten Island",
      TRUE ~ "Other"
    ))
  
  all_results <- list()
  cluster_offset <- 0
  
  # Loop through each borough group
  for (grp in unique(cuisine_data$boro_group)) {
    eps <- eps_values[[grp]]
    
    sub_data <- cuisine_data %>% filter(boro_group == grp)
    
    if (nrow(sub_data) == 0) next
    
    
    coords <- sub_data %>%
      select(longitude, latitude) %>%
      as.matrix()
    
    db_result <- dbscan::dbscan(coords, eps = eps, minPts = minPts)
    
    # Offset cluster IDs so they are unique across boroughs
    sub_data <- sub_data %>%
      mutate(cluster = ifelse(db_result$cluster == 0, 0, db_result$cluster + cluster_offset))
    
    # Update offset for next borough
    cluster_offset <- max(max(sub_data$cluster, na.rm = TRUE), cluster_offset)
    
    all_results[[grp]] <- sub_data
  }
  
  final_data <- bind_rows(all_results)
  
  return(final_data)
}

```

### 5.4 Mapping Results

To visualize the results, we map the clusters using `tmap`. Each cluster is displayed in a distinct color, enabling us to see where restaurants of a given cuisine naturally group together across the city.

``` {r mapping}
map_dbscan_clusters <- function(clustered_data) {
  
  clustered_data <- clustered_data %>%
    filter(latitude != 0, longitude != 0) %>%
    mutate(cluster = as.factor(cluster))
  
  clustered_sf <- st_as_sf(
    clustered_data,
    coords = c("longitude", "latitude"),
    crs = 4326,   # WGS84 lat/long
    remove = FALSE
  )
  
  # Dynamically create enough colors for the number of clusters
  num_clusters <- length(unique(clustered_sf$cluster))
  colors <- brewer.pal(
    n = max(3, min(12, num_clusters)),  # palettes max out at 12 colors
    name = "Set1"
  )
  
  tmap_mode("view")  # interactive mode
  
  tm_shape(clustered_sf) +
    tm_dots(
      col = "cluster",        # color by cluster ID
      palette = colors,       # distinct colors
      size = 0.55,            # small points
      alpha = 0.7,            # semi-transparent
      title = "Cluster ID"
    ) +
    tm_layout(legend.outside = TRUE)
  
}
```

### 5.5 Polygon "Region" Creation

To move beyond individual points, we create polygon boundaries around clusters. These polygons represent cuisine “hotspot regions” that summarize where in the city a cuisine is most concentrated.

Steps:
1. **Filter** out noise points (cluster `0`).
2. **Group** restaurants by cluster ID.
3. **Create convex hulls** (minimum polygons that enclose each cluster’s points).
4. **Merge overlapping hulls** to handle non-radial clusters split into multiple shapes.
5. **Smooth or buffer** polygons so they look more like real neighborhood shapes (wider than a single street line).

The result is an `sf` object where each polygon represents a cuisine hotspot, with an attribute for the number of restaurants in it.


``` {r regions}

# hotspot polygon builder
make_hotspot_polygons <- function(clustered_data,
                                  buffer_dist_m = 100,   # widen polygons by this many meters
                                  smooth = TRUE,         # apply small smoothing (buffer+unbuffer)
                                  smooth_frac = 0.5,     # smoothing distance as fraction of buffer_dist_m
                                  proj_crs = 3857) {     # projected CRS for metric ops (Web Mercator)
  # clustered_data: data.frame/tibble with longitude, latitude, cluster (0 = noise), and any other cols
  # Returns: sf with polygons in WGS84 (EPSG:4326) and a 'count' column (# restaurants in region)
  
  # 1) Make sure we have points and non-zero clusters
  clustered_sf <- st_as_sf(clustered_data,
                           coords = c("longitude", "latitude"),
                           crs = 4326,
                           remove = FALSE)
  
  pts <- clustered_sf %>% filter(!is.na(cluster) & cluster != 0)

  
  # 2) Build convex hull per DBSCAN cluster (explicit geometry creation inside summarise)
  hulls <- pts %>%
    group_by(cluster) %>%
    summarise(
      count = n(),
      geometry = st_convex_hull(st_combine(geometry)),  # make one polygon per cluster
      .groups = "drop"
    )
  
  # 3) Project to metric CRS for buffering/union
  hulls_proj <- st_transform(hulls, crs = proj_crs)
  
  # 4) Buffer (widen thin shapes)
  hulls_buffed <- st_buffer(hulls_proj, dist = buffer_dist_m)
  
  # 5) Merge overlapping buffered hulls into one or more polygons
  merged_union <- st_union(hulls_buffed)                 # returns sfc
  # Cast union result into POLYGONs (may be MULTIPOLYGON -> POLYGON pieces)
  merged_polys <- st_cast(merged_union, "POLYGON")
  merged_sf <- st_as_sf(data.frame(id = seq_along(merged_polys)), geometry = merged_polys)
  st_crs(merged_sf) <- proj_crs
  
  # 6) For each merged polygon, compute total counts by summing counts of hulls that intersect it
  inter_list <- st_intersects(merged_sf, hulls_proj)
  counts <- sapply(inter_list, function(ix) {
    if (length(ix) == 0) return(0L)
    sum(hulls_proj$count[ix], na.rm = TRUE)
  })
  merged_sf$count <- counts
  
  # 7) Optional smoothing: small positive buffer then negative buffer to round corners
  if (isTRUE(smooth) && buffer_dist_m > 0) {
    smooth_dist <- buffer_dist_m * smooth_frac
    # small positive buffer then negative buffer; do it in projected CRS
    merged_sf <- st_buffer(merged_sf, smooth_dist)
    merged_sf <- st_buffer(merged_sf, -smooth_dist)
    # if buffering removed geometry because polygon too small, keep original in that case
  }
  
  # 8) Transform back to WGS84 for mapping / returning
  result <- st_transform(merged_sf, crs = 4326)
  
  # 9) Add a human-friendly label column (optional)
  result <- result %>% mutate(label = paste0("hotspot_", id))
  
  return(result)
}

```
  
This higher-level view makes it easier to identify neighborhoods associated with particular cuisines, such as Italian restaurants in Little Italy or Greek restaurants in Astoria.  
  
### 5.6 Mapping the Regions

This function takes the hotspot polygons and maps them using `tmap` in interactive view mode.  

Features:
- Polygons are colored based on the number of restaurants (`count`), using a sequential color palette.
- If no polygons are provided (e.g., the cuisine has no clusters), the function prints a message and returns nothing — preventing errors in the report.

This provides an at-a-glance visualization of where in NYC a cuisine is most concentrated.

  
``` {r mapping regions}  
map_cluster_polygons <- function(hulls) {
  
  if (nrow(hulls) == 0) {
    print("No hotspot polygons to display.")
    return(invisible(NULL))
  }
  
  tmap_mode("view")
  tm_shape(hulls) +
  tm_polygons(col = "count", palette = "YlOrRd", alpha = 0.5, title = "Restaurants in hotspot") +
  tm_shape(st_as_sf(hulls, coords = c("longitude","latitude"), crs = 4326)) 

}
```

## 6. Example Analyses

Below is a demonstration of clustering without respect to Borough density differences and one with respect to density differences. Following this is a demonstration of the region creation for one cuisine. 

### 6.1 Sample without Borough-level clustering
``` {r example Caribbean}
caribbean_clusters <- run_dbscan_for_cuisine_auto(
  data = restaurants_unique,
  cuisine = "Caribbean",
)

# head(caribbean_clusters)

map_dbscan_clusters(caribbean_clusters)
```

### 6.2 Sample with Borough-level clustering


```{r example boro any}

eps_settings <- c(
  "Manhattan" = 0.004,
  "Brooklyn/Queens" = 0.008,
  "Bronx" = 0.007,
  "Staten Island" = 0.009
)

boro_test_cuisine <- "Latin American"

boro_test_clusters <- run_dbscan_by_borough(
  data = restaurants_unique,
  cuisine = boro_test_cuisine,
  eps_values = eps_settings,
  min_floor = 5,
  prop_factor = 0.015
)

map_dbscan_clusters(boro_test_clusters)

```

The visualization allows us to adjust parameters such as eps and minPts interactively, striking a balance between capturing meaningful neighborhoods and filtering out noise.

### 6.3 Sample of Polygon Regions

Below are example cuisine hotspots for Chinese Food (of which several large hotspots are present), and African Food (despite being repsenting a very broad region, the hostpots formed have comparatively fewer restaurants).

``` {r test region 1}

region_test1_cuisine <- "Chinese"

region_test1_clusters <- run_dbscan_by_borough(
  data = restaurants_unique,
  cuisine = region_test1_cuisine,
  eps_values = eps_settings,
  prop_factor = 0.015
)

region_test1_hulls <- make_hotspot_polygons(region_test1_clusters)

map_cluster_polygons(region_test1_hulls)
```

``` {r test region 2}

region_test2_cuisine <- "African"

region_test2_clusters <- run_dbscan_by_borough(
  data = restaurants_unique,
  cuisine = region_test2_cuisine,
  eps_values = eps_settings,
  prop_factor = 0.015
)

region_test2_hulls <- make_hotspot_polygons(region_test2_clusters)

map_cluster_polygons(region_test2_hulls)
```

Having applied the clustering and mapping workflow to multiple cuisines, we can now step back and compare results across different food types.  
By looking at both common and rare cuisines, we see how hotspot patterns shift depending on restaurant density, borough-specific geography, and the choice of DBSCAN parameters.  
The next section distills these observations into key insights, highlighting spatial trends, surprising patterns, and possible socioeconomic connections that emerged from the analysis.


## 7. Insights

Using this data, we can observe that different cuisines for different cluster patterns, with some cuisines being strongly represented in a small number of hotspots, others spread across multiple hostpots, and others still having many restaurants not in hotspots.

### 7.1 Geographically Diffuse and Concentrated Cuisines

To quantify the degree of clustering, we calculate several metrics: the share of restaurants in clusters, the share in the largest cluster, the "diffusion score", and the proportion of noise points.

We calculate diffuson score (how concentrated each cuisine is in clusters), using entropy to weigh a distribution with fewer, larger, clusters as more concentrated. To create a 'Diffusion Score' from entropy, we choose the case where no clusters / hotspots appear at 1, even though an entropy calculation might suggest that all restaurants belong to the same bin (noise).

``` {r Cuisine Clustering Score}

calc_cluster_concentration <- function(clustered_data) {
  # Keep all rows with a cluster label (0 = noise)
  df <- clustered_data %>% filter(!is.na(cluster))
  N  <- nrow(df)
  if (N == 0) {
    return(tibble(
      pct_in_clusters       = 0,
      largest_cluster_share = 0,
      diffusion_score       = 1,
      n_clusters            = 0,
      pct_noise             = 1
    ))
  }

  # Counts
  noise_n <- sum(df$cluster == 0, na.rm = TRUE)
  clust_tbl <- df %>% filter(cluster != 0) %>% count(cluster, name = "n")
  n_clusters <- nrow(clust_tbl)

  # Metrics
  pct_in_clusters <- (N - noise_n) / N
  largest_cluster_share <- if (n_clusters > 0) max(clust_tbl$n) / N else 0

  # Proportions including noise as its own bin
  props <- c(clust_tbl$n, if (noise_n > 0) noise_n else NULL) / N

  # Shannon entropy (normalized). If only one non-empty bin, define as 0.
  H  <- -sum(props * log(props))
  m  <- length(props)
  Hn <- if (m <= 1) 0 else H / log(m)

  tibble(
    pct_in_clusters       = pct_in_clusters,
    largest_cluster_share = largest_cluster_share,
    diffusion_score       = Hn,
    n_clusters            = n_clusters,
    pct_noise             = noise_n / N
  )
}

```

The following code block can be used as an example to get the scores of a single cuisine:

```{r cluster ingsight example}

conc_score_cuisine <- "Italian"

conc_clusters <- run_dbscan_by_borough(
  data = restaurants_unique,
  cuisine = conc_score_cuisine,
)

map_dbscan_clusters(conc_clusters)

calc_cluster_concentration(conc_clusters)



```

We can use the following code to get the scores of every listed cuisine:

``` {r diffusion of each cuisine}

get_cuisine_diffusion_score <- function(resto_data){
  cuisine_list <- list_cuisines(resto_data)$cuisine_description
  cuisine_diff_scores <- data.frame()
  
  map_dfr(cuisine_list, function(cui){
    this_clustering <- run_dbscan_by_borough(data=resto_data, cuisine=cui)
    this_scorecard <- calc_cluster_concentration(this_clustering)
    
    bind_cols(data.frame(cuisine = cui, stringsAsFactors = FALSE), this_scorecard)
  })
  
}

print_cuisine_diff_scores <- function(cuisine_diffusion_scores){

  cuisine_diffusion_scores %>%
    arrange(desc(diffusion_score)) %>%   # or desc(), depending on what you want
    rename(
      "Cuisine"             = cuisine,
      "% in clusters"       = pct_in_clusters,
      "% in largest cluster"= largest_cluster_share,
      "Diffusion Score"     = diffusion_score,
      "No. Clusters"        = n_clusters,
      "% noise"             = pct_noise
    ) %>%
    mutate(
      # scale diffusion score from light to dark for highlighting
      `Diffusion Score` = cell_spec(round(`Diffusion Score`, 3), "html",
                                  bold = TRUE,
                                  color = ifelse(`Diffusion Score` > 0.5, "black", "white"),
                                  background = spec_color(`Diffusion Score`, end = 0.9))
    ) %>%
  kable("html", escape = FALSE, caption = "Cuisines ordered by diffusion score (a higher score is less diffuse)") %>%
  kable_styling(full_width = FALSE, position = "center")

}

cuisine_diffusion_scores <- get_cuisine_diffusion_score(restaurants_unique)

print_cuisine_diff_scores(cuisine_diffusion_scores)


```

In the table created, we can see that Polish, Egyptian, and Russian cuisine (as well as Hotdog / Pretzel establishments) are the least diffuse throughout the five boroughs (i.e, these cuisines are the most concentrated in one or a few areas). These are the cuisines most likely to found in one or two distinct hotspots in the city, and less likely to be found in any other neighborhood.
A large number of cuisines score as being very diffuse, though for many of these cuisines, few restaurants chose to be labelled as such. Here, the data is sparse and strong conclusions about the distribution of these restaurants cannot be made. 
In contrast to the sparse data case, a number of very well represented categories find themselves near the bottom of the list. Many of these labels do not map clearly to a specific cuisine, with categories such as 'Chicken', 'Juice, Smoothies, Fruit Salads', and 'Donuts' making up three of the bottom five non-zero spots. 

## 8. Conclusion


